{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052838bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '6.0.0', 'gitVersion': 'e61bf27c2f6a83fed36e5a13c008a32d563babe2', 'modules': [], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [6, 0, 0, 0], 'openssl': {'running': 'OpenSSL 1.1.1s  1 Nov 2022', 'compiled': 'OpenSSL 1.1.1f  31 Mar 2020'}, 'buildEnvironment': {'distmod': 'ubuntu2004', 'distarch': 'x86_64', 'cc': '/opt/mongodbtoolchain/v3/bin/gcc: gcc (GCC) 8.5.0', 'ccflags': '-Werror -include mongo/platform/basic.h -fasynchronous-unwind-tables -ggdb -Wall -Wsign-compare -Wno-unknown-pragmas -Winvalid-pch -fno-omit-frame-pointer -fno-strict-aliasing -O2 -march=sandybridge -mtune=generic -mprefer-vector-width=128 -Wno-unused-local-typedefs -Wno-unused-function -Wno-deprecated-declarations -Wno-unused-const-variable -Wno-unused-but-set-variable -Wno-missing-braces -fstack-protector-strong -Wa,--nocompress-debug-sections -fno-builtin-memcmp', 'cxx': '/opt/mongodbtoolchain/v3/bin/g++: g++ (GCC) 8.5.0', 'cxxflags': '-Woverloaded-virtual -Wno-maybe-uninitialized -fsized-deallocation -std=c++17', 'linkflags': '-Wl,--fatal-warnings -pthread -Wl,-z,now -fuse-ld=gold -fstack-protector-strong -Wl,--no-threads -Wl,--build-id -Wl,--hash-style=gnu -Wl,-z,noexecstack -Wl,--warn-execstack -Wl,-z,relro -Wl,--compress-debug-sections=none -Wl,-z,origin -Wl,--enable-new-dtags', 'target_arch': 'x86_64', 'target_os': 'linux', 'cppdefines': 'SAFEINT_USE_INTRINSICS 0 PCRE_STATIC NDEBUG _XOPEN_SOURCE 700 _GNU_SOURCE _FORTIFY_SOURCE 2 BOOST_THREAD_VERSION 5 BOOST_THREAD_USES_DATETIME BOOST_SYSTEM_NO_DEPRECATED BOOST_MATH_NO_LONG_DOUBLE_MATH_FUNCTIONS BOOST_ENABLE_ASSERT_DEBUG_HANDLER BOOST_LOG_NO_SHORTHAND_NAMES BOOST_LOG_USE_NATIVE_SYSLOG BOOST_LOG_WITHOUT_THREAD_ATTR ABSL_FORCE_ALIGNED_ACCESS'}, 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'wiredTiger'], 'ok': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 12:55:09.209476: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 12:55:09.209507: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting game creation thread...\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 101uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 12:55:12.187375] Game created\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 101uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 12:56:14.446650] Game created\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 101uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 12:57:16.589650] Game created\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 101uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 12:58:19.370831] Game created\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 100uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 12:59:21.382602] Game created\n",
      "response: {\"message\":\"TrainingGame sa id-ijem: 100uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 13:00:23.557088] Game created\n"
     ]
    }
   ],
   "source": [
    "import db\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from serializer import GameState1DSerializer\n",
    "import sys\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import json\n",
    "import random as rd\n",
    "import agent_wrapper\n",
    "\n",
    "action_map = {\n",
    "    0: \"move,1,1\",\n",
    "}\n",
    "\n",
    "action_map_inverse = {v:k for k,v in action_map.items()}\n",
    "\n",
    "def prepare_training_data(from_timestamp = 0):\n",
    "    \n",
    "    if(isinstance(from_timestamp, dt.datetime)):\n",
    "        from_timestamp = int(from_timestamp.timestamp())\n",
    "        \n",
    "    replays = db.get_all_experiences({ \"time\": { \"$gt\": from_timestamp}})\n",
    "    \n",
    "    def _get_score_from_state(state: dict):\n",
    "        try:\n",
    "            state = json.loads(state[\"gameState\"])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        for key in [\"player1\", \"player2\", \"player3\", \"player4\"]:\n",
    "            if state[key][\"name\"] == \"JutricKafica\":\n",
    "                return state[key][\"score\"]\n",
    "    \n",
    "    \n",
    "    rewards = [_get_score_from_state(replay[\"sp\"]) - _get_score_from_state(replay[\"s\"]) for replay in replays]\n",
    "\n",
    "    _seralizer = GameState1DSerializer()\n",
    "\n",
    "    serialized = [\n",
    "        _seralizer.serialize_single(x) for x in replays\n",
    "    ]\n",
    "    \n",
    "    actions = [\n",
    "        replay['a'] for replay in replays\n",
    "    ]\n",
    "\n",
    "    return serialized, rewards, actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_targets(model, training_data, rewards):\n",
    "    \n",
    "    n = len(training_data)\n",
    "    model_inputs = []\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        model_inputs.append(np.hstack(training_data[i], training_data[i-1], training_data[i-2]))\n",
    "    \n",
    "    states = model_inputs[:-1]\n",
    "    next_states = model_inputs[1:]\n",
    "    \n",
    "    return states, actions[2:-1], next_states, rewards[2:-1]\n",
    "    \n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.espilon_decay = 0.995\n",
    "        self.update_rate = 300\n",
    "        \n",
    "        self.model = self._build_model(state_size, action_size)\n",
    "        self.target_model = self._build_model(state_size, action_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.last_updated = int(dt.datetime().now().timestamp())\n",
    "    \n",
    "    \n",
    "    def _build_model(self, state_size, action_size):\n",
    "        \n",
    "        # Define the model architecture\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(state_size,)))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "\n",
    "        # Compile the model with an optimizer and a loss function\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return action_map(np.argmax(act_values[0]))  # Returns action using polic\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        \n",
    "        states, actions, next_states, rewards = create_targets(*prepare_training_data(self.last_updated))\n",
    "        \n",
    "        data = [states, actions, next_states, rewards]\n",
    "        \n",
    "        rd.shuffle(data)\n",
    "        \n",
    "        for state, action, next_state, reward in zip(*data):\n",
    "            \n",
    "            target = reward + self.gamma * np.amax(self.target_model.predict(next_state))\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action_map_invere(action)] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose = 0)\n",
    "            \n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.last_updated = int(dt.datetime.now().timestamp())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa008d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               240128    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 40)                2600      \n",
      "=================================================================\n",
      "Total params: 250,984\n",
      "Trainable params: 250,984\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "function missing required argument 'year' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(serialized)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_state_space())\n\u001b[0;32m----> 7\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_state_space\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m timestep \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, state_size, action_size)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model\u001b[38;5;241m.\u001b[39mset_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_weights())\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mdt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mtimestamp())\n",
      "\u001b[0;31mTypeError\u001b[0m: function missing required argument 'year' (pos 1)"
     ]
    }
   ],
   "source": [
    "def get_state_space():\n",
    "    one_document = db.replay_buffer_collection.find_one()\n",
    "    serialized = GameState1DSerializer().serialize_single(one_document)\n",
    "    return len(serialized)\n",
    "\n",
    "agent = DQNAgent(3 * get_state_space(), 40)\n",
    "\n",
    "\n",
    "train = True\n",
    "timestep = 0\n",
    "\n",
    "initial_state = json.loads(\"../initial_state.json\")\n",
    "\n",
    "\n",
    "print(agent.update())\n",
    "\n",
    "# while train:\n",
    "    \n",
    "#     if timestep != 0 and timestep % self.update_rate == 0:\n",
    "#\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37642890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
