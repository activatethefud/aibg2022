{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "052838bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': '6.0.0', 'gitVersion': 'e61bf27c2f6a83fed36e5a13c008a32d563babe2', 'modules': [], 'allocator': 'tcmalloc', 'javascriptEngine': 'mozjs', 'sysInfo': 'deprecated', 'versionArray': [6, 0, 0, 0], 'openssl': {'running': 'OpenSSL 1.1.1s  1 Nov 2022', 'compiled': 'OpenSSL 1.1.1f  31 Mar 2020'}, 'buildEnvironment': {'distmod': 'ubuntu2004', 'distarch': 'x86_64', 'cc': '/opt/mongodbtoolchain/v3/bin/gcc: gcc (GCC) 8.5.0', 'ccflags': '-Werror -include mongo/platform/basic.h -fasynchronous-unwind-tables -ggdb -Wall -Wsign-compare -Wno-unknown-pragmas -Winvalid-pch -fno-omit-frame-pointer -fno-strict-aliasing -O2 -march=sandybridge -mtune=generic -mprefer-vector-width=128 -Wno-unused-local-typedefs -Wno-unused-function -Wno-deprecated-declarations -Wno-unused-const-variable -Wno-unused-but-set-variable -Wno-missing-braces -fstack-protector-strong -Wa,--nocompress-debug-sections -fno-builtin-memcmp', 'cxx': '/opt/mongodbtoolchain/v3/bin/g++: g++ (GCC) 8.5.0', 'cxxflags': '-Woverloaded-virtual -Wno-maybe-uninitialized -fsized-deallocation -std=c++17', 'linkflags': '-Wl,--fatal-warnings -pthread -Wl,-z,now -fuse-ld=gold -fstack-protector-strong -Wl,--no-threads -Wl,--build-id -Wl,--hash-style=gnu -Wl,-z,noexecstack -Wl,--warn-execstack -Wl,-z,relro -Wl,--compress-debug-sections=none -Wl,-z,origin -Wl,--enable-new-dtags', 'target_arch': 'x86_64', 'target_os': 'linux', 'cppdefines': 'SAFEINT_USE_INTRINSICS 0 PCRE_STATIC NDEBUG _XOPEN_SOURCE 700 _GNU_SOURCE _FORTIFY_SOURCE 2 BOOST_THREAD_VERSION 5 BOOST_THREAD_USES_DATETIME BOOST_SYSTEM_NO_DEPRECATED BOOST_MATH_NO_LONG_DOUBLE_MATH_FUNCTIONS BOOST_ENABLE_ASSERT_DEBUG_HANDLER BOOST_LOG_NO_SHORTHAND_NAMES BOOST_LOG_USE_NATIVE_SYSLOG BOOST_LOG_WITHOUT_THREAD_ATTR ABSL_FORCE_ALIGNED_ACCESS'}, 'bits': 64, 'debug': False, 'maxBsonObjectSize': 16777216, 'storageEngines': ['devnull', 'ephemeralForTest', 'wiredTiger'], 'ok': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 14:28:51.797007: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-10 14:28:51.797038: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting game creation thread...\n"
     ]
    }
   ],
   "source": [
    "import db\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from serializer import GameState1DSerializer\n",
    "import sys\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import json\n",
    "import random as rd\n",
    "import agent_wrapper\n",
    "import randomAgents\n",
    "\n",
    "action_map = {}\n",
    "counter = 0\n",
    "\n",
    "for action in [\"attack\", \"move\"]:\n",
    "    for q in range(-14,15):\n",
    "        for r in range(-14, 15):\n",
    "            action_map[counter] = f\"{action},{q},{r}\"\n",
    "            counter += 1\n",
    "            \n",
    "\n",
    "action_map_inverse = {v:k for k,v in action_map.items()}\n",
    "\n",
    "#print(action_map, action_map_inverse)\n",
    "\n",
    "def prepare_training_data(from_timestamp = 0):\n",
    "    \n",
    "    if(isinstance(from_timestamp, dt.datetime)):\n",
    "        from_timestamp = int(from_timestamp.timestamp())\n",
    "        \n",
    "    replays = db.get_all_experiences({ \"time\": { \"$gt\": from_timestamp}})\n",
    "    \n",
    "    def _get_score_from_state(state: dict):\n",
    "        try:\n",
    "            state = json.loads(state[\"gameState\"])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        for key in [\"player1\", \"player2\", \"player3\", \"player4\"]:\n",
    "            if state[key][\"name\"] == \"JutricKafica\":\n",
    "                return state[key][\"score\"]\n",
    "    \n",
    "    \n",
    "    rewards = [_get_score_from_state(replay[\"sp\"]) - _get_score_from_state(replay[\"s\"]) for replay in replays]\n",
    "\n",
    "    _seralizer = GameState1DSerializer()\n",
    "\n",
    "    serialized = [\n",
    "        _seralizer.serialize_single(x) for x in replays\n",
    "    ]\n",
    "    \n",
    "    actions = [\n",
    "        replay['a'] for replay in replays\n",
    "    ]\n",
    "\n",
    "    return serialized, rewards, actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_targets(training_data, rewards, actions):\n",
    "    \n",
    "    n = len(training_data)\n",
    "    model_inputs = []\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        model_inputs.append(np.hstack(training_data[i], training_data[i-1], training_data[i-2]))\n",
    "    \n",
    "    states = model_inputs[:-1]\n",
    "    next_states = model_inputs[1:]\n",
    "    \n",
    "    return states, actions[2:-1], next_states, rewards[2:-1]\n",
    "    \n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        print(type(state_size), type(action_size))\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.update_rate = 300\n",
    "        \n",
    "        self.model = self._build_model(state_size, action_size)\n",
    "        self.target_model = self._build_model(state_size, action_size)\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.last_updated = int((dt.datetime.now() - dt.timedelta(hours = 1)).timestamp())\n",
    "    \n",
    "    \n",
    "    def _build_model(self, state_size, action_size):\n",
    "        \n",
    "        # Define the model architecture\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(state_size,)))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "\n",
    "        # Compile the model with an optimizer and a loss function\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def act(self, state, state_json):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            picked_random_valid = randomAgents.pick_rand_action(state_json)\n",
    "            return f\"{picked_random_valid[0]},{picked_random_valid[1]},{picked_random_valid[2]}\"\n",
    "        \n",
    "        act_values = self.model.predict(state)[0]\n",
    "        \n",
    "        for idx,elem in enumerate(act_values):\n",
    "            action = action_map[elem]\n",
    "            action_tokens = action.split(\",\")\n",
    "            if not randomAgents.is_valid_action(action_tokens[0], action_tokens[1], action_tokens[2], state_json):\n",
    "                act_values[idx] = 0\n",
    "        \n",
    "        return action_map[np.argmax(act_values[0])]  # Returns action using polic\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        \n",
    "        states, actions, next_states, rewards = create_targets(*prepare_training_data(self.last_updated))\n",
    "        \n",
    "        data = [states, actions, next_states, rewards]\n",
    "        \n",
    "        print(data)\n",
    "        \n",
    "        rd.shuffle(data)\n",
    "        \n",
    "        for state, action, next_state, reward in zip(*data):\n",
    "            \n",
    "            target = reward + self.gamma * np.amax(self.target_model.predict(next_state))\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action_map_invere(action)] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose = 0)\n",
    "            \n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.last_updated = int(dt.datetime.now().timestamp())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa008d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'> <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-10 14:28:53.856634: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-12-10 14:28:53.856671: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-12-10 14:28:53.856691: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (nikola-tpyoga): /proc/driver/nvidia/version does not exist\n",
      "2022-12-10 14:28:53.856901: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: {\"message\":\"TrainingGame sa id-ijem: 100uspeÅ¡no napravljen.\",\"gameState\":\"{\\\"map\\\":{\\\"size\\\":29,\\\"tiles\\\":[[{\\\"q\\\":0,\\\"r\\\":-14,\\\"entity\\\":{\\\"type\\\":\\\n",
      "[2022-12-10 14:28:54.766675] Game created\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               240128    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1682)              109330    \n",
      "=================================================================\n",
      "Total params: 357,714\n",
      "Trainable params: 357,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "attack,-7,-6\n",
      "attack,-7,-5\n",
      "move,-8,-6\n",
      "attack,-10,-7\n",
      "move,-9,-5\n",
      "attack,-12,-6\n",
      "move,-8,-5\n",
      "move,-9,-5\n",
      "attack,-7,-4\n",
      "move,-8,-4\n",
      "attack,-12,-6\n",
      "attack,-12,-2\n",
      "attack,-10,-6\n",
      "attack,-10,-8\n",
      "move,-9,-4\n",
      "move,-9,-5\n",
      "attack,-9,-4\n",
      "move,-8,-6\n",
      "attack,-5,-7\n",
      "attack,-9,-6\n",
      "attack,-7,-7\n",
      "move,-8,-7\n",
      "move,-6,-8\n",
      "attack,-6,-8\n",
      "move,-7,-9\n",
      "attack,-6,-5\n",
      "attack,-6,-7\n",
      "attack,-6,-6\n",
      "move,-5,-8\n",
      "move,-7,-7\n",
      "attack,-8,-7\n",
      "attack,-4,-6\n",
      "attack,-9,-5\n",
      "move,-7,-8\n",
      "move,-6,-7\n",
      "attack,-4,-5\n",
      "attack,-5,-7\n",
      "move,-6,-8\n",
      "move,-6,-9\n",
      "attack,-6,-6\n",
      "attack,-3,-7\n",
      "move,-5,-9\n",
      "move,-7,-8\n",
      "move,-7,-7\n",
      "attack,-8,-9\n",
      "attack,-4,-7\n",
      "move,-7,-7\n",
      "attack,-4,-6\n",
      "move,-6,-8\n",
      "move,-6,-8\n",
      "attack,-4,-7\n",
      "attack,-5,-5\n",
      "attack,-4,-5\n",
      "move,-5,-9\n",
      "move,-6,-7\n",
      "attack,-6,-9\n",
      "move,-7,-7\n",
      "move,-6,-7\n",
      "attack,-4,-9\n",
      "move,-7,-6\n",
      "attack,-5,-3\n",
      "attack,-5,-4\n",
      "move,-6,-6\n",
      "move,-6,-5\n",
      "attack,-5,-9\n",
      "move,-8,-5\n",
      "move,-6,-7\n",
      "move,-7,-8\n",
      "move,-5,-6\n",
      "attack,-9,-9\n",
      "attack,-4,-5\n",
      "attack,-6,-7\n",
      "attack,-9,-9\n",
      "attack,-5,-9\n",
      "attack,-9,-8\n",
      "attack,-9,-9\n",
      "attack,-5,-9\n",
      "attack,-4,-5\n",
      "attack,-9,-7\n",
      "move,-6,-8\n",
      "attack,-8,-9\n",
      "attack,-4,-6\n",
      "attack,-6,-10\n",
      "move,-6,-8\n",
      "move,-7,-6\n",
      "move,-6,-7\n",
      "attack,-4,-7\n"
     ]
    }
   ],
   "source": [
    "def get_state_space():\n",
    "    one_document = db.replay_buffer_collection.find_one()\n",
    "    serialized = GameState1DSerializer().serialize_single(one_document)\n",
    "    return len(serialized)\n",
    "\n",
    "agent = DQNAgent(3 * get_state_space(), len(action_map))\n",
    "\n",
    "#agent.update()\n",
    "\n",
    "train = True\n",
    "timestep = 0\n",
    "\n",
    "initial_obs = json.load(open(\"../initial_state.json\",'r'))\n",
    "\n",
    "obs_window = 3*[initial_obs]\n",
    "\n",
    "while train:\n",
    "    \n",
    "    if timestep != 0 and timestep % agent.update_rate == 0:\n",
    "        agent.update()\n",
    "        \n",
    "    try:\n",
    "        state = np.hstack([GameState1DSerializer().serialize_single(x) for x in obs_window])\n",
    "    except:\n",
    "        from pprint import pprint\n",
    "        pprint(obs_window)\n",
    "\n",
    "    action = agent.act(state, obs_window[-1])\n",
    "    \n",
    "    print(action)\n",
    "    \n",
    "    _split = action.split(\",\")\n",
    "    mode = _split[0]\n",
    "    x = _split[1]\n",
    "    y = _split[2]\n",
    "\n",
    "    if mode == \"attack\":\n",
    "        info, success = agent_wrapper.attack(\"DQN\", obs_window[-1],x,y)\n",
    "    else:\n",
    "        info, success = agent_wrapper.move(\"DQN\", obs_window[-1],x,y)\n",
    "        \n",
    "    if(success):\n",
    "        new_obs = info\n",
    "        obs_window.append(new_obs)\n",
    "        del obs_window[0]\n",
    "\n",
    "        \n",
    "    timestep += 1\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938375e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a7a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
