{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "052838bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import db\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from serializer import GameState1DSerializer\n",
    "import sys\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import json\n",
    "import random as rd\n",
    "\n",
    "gamma = 0.99\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995\n",
    "update_rate = 500\n",
    "\n",
    "\n",
    "action_map = {\n",
    "    0: \"move,1,1\",\n",
    "}\n",
    "\n",
    "action_map_inverse = {v:k for k,v in action_map.items()}\n",
    "\n",
    "def prepare_training_data(from_timestamp = 0):\n",
    "    \n",
    "    if(isinstance(from_timestamp, dt.datetime)):\n",
    "        from_timestamp = int(from_timestamp.timestamp())\n",
    "        \n",
    "    replays = db.get_all_experiences({ \"time\": { \"$gt\": from_timestamp}})\n",
    "    \n",
    "    def _get_score_from_state(state: dict):\n",
    "        try:\n",
    "            state = json.loads(state[\"gameState\"])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        for key in [\"player1\", \"player2\", \"player3\", \"player4\"]:\n",
    "            if state[key][\"name\"] == \"JutricKafica\":\n",
    "                return state[key][\"score\"]\n",
    "    \n",
    "    \n",
    "    rewards = [_get_score_from_state(replay[\"sp\"]) - _get_score_from_state(replay[\"s\"]) for replay in replays]\n",
    "\n",
    "    _seralizer = GameState1DSerializer()\n",
    "\n",
    "    serialized = [\n",
    "        _seralizer.serialize_single(x) for x in replays\n",
    "    ]\n",
    "    \n",
    "    actions = [\n",
    "        replay['a'] for replay in replays\n",
    "    ]\n",
    "\n",
    "    return serialized, rewards, actions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_targets(model, training_data, rewards):\n",
    "    \n",
    "    n = len(training_data)\n",
    "    model_inputs = []\n",
    "    \n",
    "    for i in range(2, n):\n",
    "        model_inputs.append(np.hstack(training_data[i], training_data[i-1], training_data[i-2]))\n",
    "    \n",
    "    states = model_inputs[:-1]\n",
    "    next_states = model_inputs[1:]\n",
    "    \n",
    "    return states, actions[2:-1], next_states, rewards[2:-1]\n",
    "    \n",
    "    \n",
    "\n",
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, state_size, action_size):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_min = 0.1\n",
    "        self.espilon_decay = 0.995\n",
    "        self.update_rate = 300\n",
    "        \n",
    "        self.model = self._build_model(state_size, action_size)\n",
    "        self.target_model = self._build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.last_updated = int(dt.datetime().now().timestamp())\n",
    "    \n",
    "    \n",
    "    def _build_model(self, state_size, action_size):\n",
    "        \n",
    "        # Define the model architecture\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Input(shape=(state_size,)))\n",
    "        model.add(keras.layers.Dense(128, activation='relu'))\n",
    "        model.add(keras.layers.Dense(64, activation='relu'))\n",
    "        model.add(keras.layers.Dense(action_size, activation='linear'))\n",
    "\n",
    "        # Compile the model with an optimizer and a loss function\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.predict(state)\n",
    "        \n",
    "        return action_map(np.argmax(act_values[0]))  # Returns action using polic\n",
    "    \n",
    "    \n",
    "    def update(self):\n",
    "        \n",
    "        states, actions, next_states, rewards = create_targets(*prepare_training_data(self.last_updated))\n",
    "        \n",
    "        data = [states, actions, next_states, rewards]\n",
    "        \n",
    "        rd.shuffle(data)\n",
    "        \n",
    "        for state, action, next_state, reward in zip(*data):\n",
    "            \n",
    "            target = reward + self.gamma * np.amax(self.target_model.predict(next_state))\n",
    "            \n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action_map_invere(action)] = target\n",
    "            \n",
    "            self.model.fit(state, target_f, epochs=1, verbose = 0)\n",
    "            \n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.last_updated = int(dt.datetime.now().timestamp())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa008d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1418855980.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [23]\u001b[0;36m\u001b[0m\n\u001b[0;31m    if timestep != 0\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "\n",
    "\n",
    "train = True\n",
    "update_freq = 300\n",
    "timestep = 0\n",
    "\n",
    "while train:\n",
    "    \n",
    "    if timestep != 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53defc50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
